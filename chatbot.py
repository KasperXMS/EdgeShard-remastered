from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
import re
from llama import Llama
from torch.distributed import rpc
import os, yaml


# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## DeepSeek-R1-Distill-LLaMA-8B LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨ 0 åˆ° 8192 ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º 8192ï¼ˆDeepSeek-R1-Distill-Qwen-7B æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œå¹¶èƒ½ç”Ÿæˆæœ€å¤š 8K tokensï¼Œæˆ‘ä»¬æ¨èè®¾ä¸º 8192ï¼Œå› ä¸ºæ€è€ƒéœ€è¦è¾“å‡ºæ›´å¤šçš„Tokenæ•°ï¼‰
    max_length = st.slider("max_length", 0, 512, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ DeepSeek R1 Distill Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = '/root/autodl-tmp/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'

# æ–‡æœ¬åˆ†å‰²å‡½æ•°
def split_text(text):
    pattern = re.compile(r'<think>(.*?)</think>(.*)', re.DOTALL) # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    match = pattern.search(text) # åŒ¹é… <think>æ€è€ƒè¿‡ç¨‹</think>å›ç­”
  
    if match: # å¦‚æœåŒ¹é…åˆ°æ€è€ƒè¿‡ç¨‹
        think_content = match.group(1).strip() # è·å–æ€è€ƒè¿‡ç¨‹
        answer_content = match.group(2).strip() # è·å–å›ç­”
    else:
        think_content = "" # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ€è€ƒè¿‡ç¨‹ï¼Œåˆ™è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
        answer_content = text.strip() # ç›´æ¥è¿”å›å›ç­”
  
    return think_content, answer_content

# # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œ tokenizer
# @st.cache_resource
# def get_model():
#     # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å– tokenizer
#     tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
#     tokenizer.pad_token = tokenizer.eos_token
#     # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
#     model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.bfloat16,  device_map="auto")

#     return tokenizer, model

# # åŠ è½½ Qwen2.5 çš„ model å’Œ tokenizer
# tokenizer, model = get_model()

@st.cache_resource
def get_generator(
    ckpt_dir: str,
    tokenizer_path: str,
    temperature: float = 0.3,
    top_p: float = 0.15,
    max_seq_len: int = 2048,
    max_batch_size: int = 1,
    max_gen_len: int = 256,
    ):
    
    config = {}
    with open("config/config.yaml", 'r') as file:
        config = yaml.safe_load(file)

    os.environ['GLOO_SOCKET_IFNAME'] = config['master']['interface']
    os.environ['TP_SOCKET_IFNAME'] = config['master']['interface']
    os.environ['MASTER_ADDR'] = config['master']['ip']
    os.environ['MASTER_PORT'] = config['master']['port']

    rpc.init_rpc("master", rank=0, world_size=len(config['worker'])+1, 
                 rpc_backend_options=rpc.TensorPipeRpcBackendOptions(
                rpc_timeout=120))

    generator = Llama.build_distributed(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
    )
    return generator

generator = get_generator(
        ckpt_dir="./model_shards",
        tokenizer_path="./llama_3.1_8b_tokenizer/tokenizer.model",
)

# å¦‚æœ session_state ä¸­æ²¡æœ‰ "messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

# éå† session_state ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages = [{"role": "user", "content": prompt}]

    print(st.session_state.messages)

    results = generator.chat_completion(
        [st.session_state.messages], max_gen_len=256
    )
    for result in results:
        response = result['generation']['content']
        think_content, answer_content = split_text(response) # è°ƒç”¨split_textå‡½æ•°ï¼Œåˆ†å‰²æ€è€ƒè¿‡ç¨‹å’Œå›ç­”
        # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    with st.expander("æ¨¡å‹æ€è€ƒè¿‡ç¨‹"):
        st.write(think_content) # å±•ç¤ºæ¨¡å‹æ€è€ƒè¿‡ç¨‹
    st.chat_message("assistant").write(answer_content) # è¾“å‡ºæ¨¡å‹å›ç­”
    # print(st.session_state) # æ‰“å° session_state è°ƒè¯• 
